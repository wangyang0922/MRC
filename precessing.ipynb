{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append('/Users/sara/Documents/算法学习/NLP/nlp名企/MRC/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from utils.data_loader import sentence_proc,sentences_proc,en_split_sentences_proc,documents_proc,split_sentences_proc,split_sentences_proc\n",
    "from utils.file_utils import save_dict\n",
    "from utils.multi_proc_utils import parallelize, cores\n",
    "from utils.config import search_dev_data_path, zhidao_dev_data_path, merger_dev_seg_path,squad2_vocab_path,save_squad2_wv_model_path,squad_dev_data_path,squad2_merger_dev_seg_path,stop_word_path,save_wv_model_path,vocab_path,embedding_matrix_path,search_train_data_path, merger_seg_path,zhidao_train_data_path, search_test_data_path, zhidao_test_data_path,embedding_dim,wv_train_epochs\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 Dureader2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search dev data size 5000,zhidao dev data size 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:15:36,075 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:15:36,090 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:15:36,126 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:15:36,137 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:15:36,187 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:15:36,198 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:15:36,251 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:15:36,266 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:15:36,333 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:15:36,345 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:15:36,413 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:15:36,428 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:15:36,511 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:15:36,521 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:15:36,583 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:15:36,592 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Loading model cost 1.833 seconds.\n",
      "Loading model cost 2.094 seconds.\n",
      "Loading model cost 1.751 seconds.\n",
      "Loading model cost 1.913 seconds.\n",
      "Loading model cost 2.041 seconds.\n",
      "Loading model cost 1.982 seconds.\n",
      "2020-09-20 09:15:38,177 : DEBUG : Loading model cost 2.094 seconds.\n",
      "2020-09-20 09:15:38,177 : DEBUG : Loading model cost 1.833 seconds.\n",
      "2020-09-20 09:15:38,177 : DEBUG : Loading model cost 1.913 seconds.\n",
      "2020-09-20 09:15:38,178 : DEBUG : Loading model cost 1.982 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,177 : DEBUG : Loading model cost 2.041 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,181 : DEBUG : Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,181 : DEBUG : Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,182 : DEBUG : Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,182 : DEBUG : Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,183 : DEBUG : Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,177 : DEBUG : Loading model cost 1.751 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,240 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.761 seconds.\n",
      "2020-09-20 09:15:38,352 : DEBUG : Loading model cost 1.761 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,363 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.846 seconds.\n",
      "2020-09-20 09:15:38,366 : DEBUG : Loading model cost 1.846 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:15:38,383 : DEBUG : Prefix dict has been built succesfully.\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:16:25,048 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:16:25,061 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:16:25,105 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:16:25,115 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:16:25,170 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:16:25,186 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:16:25,238 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:16:25,253 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:16:25,300 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:16:25,316 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:16:25,369 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:16:25,383 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:16:25,453 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:16:25,469 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:16:25,539 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:16:25,553 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Loading model cost 1.700 seconds.\n",
      "2020-09-20 09:16:26,755 : DEBUG : Loading model cost 1.700 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:16:26,775 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.732 seconds.\n",
      "2020-09-20 09:16:26,845 : DEBUG : Loading model cost 1.732 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:16:26,850 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.748 seconds.\n",
      "2020-09-20 09:16:26,929 : DEBUG : Loading model cost 1.748 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:16:26,939 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.740 seconds.\n",
      "2020-09-20 09:16:26,990 : DEBUG : Loading model cost 1.740 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:16:27,000 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.736 seconds.\n",
      "2020-09-20 09:16:27,048 : DEBUG : Loading model cost 1.736 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:16:27,058 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.707 seconds.\n",
      "2020-09-20 09:16:27,087 : DEBUG : Loading model cost 1.707 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:16:27,097 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.696 seconds.\n",
      "2020-09-20 09:16:27,162 : DEBUG : Loading model cost 1.696 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:16:27,174 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.666 seconds.\n",
      "2020-09-20 09:16:27,215 : DEBUG : Loading model cost 1.666 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:16:27,226 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size 5000,test data size 5000,merged_df data size 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-20 09:17:07,296 : INFO : collecting all words and their counts\n",
      "2020-09-20 09:17:07,298 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start build w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-20 09:17:12,525 : INFO : PROGRESS: at sentence #10000, processed 16137801 words, keeping 265617 word types\n",
      "2020-09-20 09:17:12,540 : INFO : collected 265617 word types from a corpus of 16181598 raw words and 10032 sentences\n",
      "2020-09-20 09:17:12,541 : INFO : Loading a fresh vocabulary\n",
      "2020-09-20 09:17:12,950 : INFO : effective_min_count=5 retains 114731 unique words (43% of original 265617, drops 150886)\n",
      "2020-09-20 09:17:12,951 : INFO : effective_min_count=5 leaves 15796392 word corpus (97% of original 16181598, drops 385206)\n",
      "2020-09-20 09:17:13,330 : INFO : deleting the raw counts dictionary of 265617 items\n",
      "2020-09-20 09:17:13,338 : INFO : sample=0.001 downsamples 20 most-common words\n",
      "2020-09-20 09:17:13,339 : INFO : downsampling leaves estimated 13672312 word corpus (86.6% of prior 15796392)\n",
      "2020-09-20 09:17:13,709 : INFO : estimated required memory for 114731 words and 300 dimensions: 332719900 bytes\n",
      "2020-09-20 09:17:13,710 : INFO : resetting layer weights\n",
      "2020-09-20 09:17:35,823 : INFO : training model with 8 workers on 114731 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-09-20 09:17:36,847 : INFO : EPOCH 1 - PROGRESS: at 1.96% examples, 255421 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:37,900 : INFO : EPOCH 1 - PROGRESS: at 4.24% examples, 278308 words/s, in_qsize 14, out_qsize 2\n",
      "2020-09-20 09:17:38,951 : INFO : EPOCH 1 - PROGRESS: at 6.46% examples, 280551 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:39,954 : INFO : EPOCH 1 - PROGRESS: at 8.89% examples, 289935 words/s, in_qsize 14, out_qsize 0\n",
      "2020-09-20 09:17:40,990 : INFO : EPOCH 1 - PROGRESS: at 11.16% examples, 290819 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:42,007 : INFO : EPOCH 1 - PROGRESS: at 13.32% examples, 291659 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:43,099 : INFO : EPOCH 1 - PROGRESS: at 15.71% examples, 292504 words/s, in_qsize 16, out_qsize 0\n",
      "2020-09-20 09:17:44,099 : INFO : EPOCH 1 - PROGRESS: at 17.89% examples, 293866 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:45,130 : INFO : EPOCH 1 - PROGRESS: at 20.15% examples, 291389 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:46,166 : INFO : EPOCH 1 - PROGRESS: at 22.25% examples, 291385 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:47,180 : INFO : EPOCH 1 - PROGRESS: at 24.33% examples, 291298 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:48,182 : INFO : EPOCH 1 - PROGRESS: at 26.56% examples, 291967 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:49,192 : INFO : EPOCH 1 - PROGRESS: at 28.74% examples, 292456 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:50,241 : INFO : EPOCH 1 - PROGRESS: at 30.99% examples, 291881 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:51,252 : INFO : EPOCH 1 - PROGRESS: at 33.25% examples, 292885 words/s, in_qsize 16, out_qsize 0\n",
      "2020-09-20 09:17:52,269 : INFO : EPOCH 1 - PROGRESS: at 35.47% examples, 293737 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:53,291 : INFO : EPOCH 1 - PROGRESS: at 37.61% examples, 293533 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:54,355 : INFO : EPOCH 1 - PROGRESS: at 39.82% examples, 292855 words/s, in_qsize 14, out_qsize 1\n",
      "2020-09-20 09:17:55,391 : INFO : EPOCH 1 - PROGRESS: at 41.86% examples, 292891 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:56,414 : INFO : EPOCH 1 - PROGRESS: at 44.07% examples, 293117 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:57,460 : INFO : EPOCH 1 - PROGRESS: at 46.38% examples, 293553 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:58,474 : INFO : EPOCH 1 - PROGRESS: at 48.49% examples, 293413 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:17:59,486 : INFO : EPOCH 1 - PROGRESS: at 51.00% examples, 294213 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:00,494 : INFO : EPOCH 1 - PROGRESS: at 53.12% examples, 294080 words/s, in_qsize 15, out_qsize 1\n",
      "2020-09-20 09:18:01,505 : INFO : EPOCH 1 - PROGRESS: at 55.30% examples, 294412 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:02,512 : INFO : EPOCH 1 - PROGRESS: at 57.57% examples, 294087 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:03,558 : INFO : EPOCH 1 - PROGRESS: at 59.77% examples, 294153 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:04,574 : INFO : EPOCH 1 - PROGRESS: at 61.99% examples, 293920 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:05,606 : INFO : EPOCH 1 - PROGRESS: at 63.99% examples, 293114 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:06,623 : INFO : EPOCH 1 - PROGRESS: at 66.14% examples, 293034 words/s, in_qsize 14, out_qsize 1\n",
      "2020-09-20 09:18:07,629 : INFO : EPOCH 1 - PROGRESS: at 68.36% examples, 293282 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:08,683 : INFO : EPOCH 1 - PROGRESS: at 70.81% examples, 293552 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:09,698 : INFO : EPOCH 1 - PROGRESS: at 72.83% examples, 293499 words/s, in_qsize 14, out_qsize 1\n",
      "2020-09-20 09:18:10,758 : INFO : EPOCH 1 - PROGRESS: at 75.07% examples, 293632 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:11,812 : INFO : EPOCH 1 - PROGRESS: at 77.49% examples, 293736 words/s, in_qsize 15, out_qsize 1\n",
      "2020-09-20 09:18:12,825 : INFO : EPOCH 1 - PROGRESS: at 79.75% examples, 293994 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:13,832 : INFO : EPOCH 1 - PROGRESS: at 81.83% examples, 293857 words/s, in_qsize 16, out_qsize 0\n",
      "2020-09-20 09:18:14,843 : INFO : EPOCH 1 - PROGRESS: at 84.10% examples, 294044 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:15,877 : INFO : EPOCH 1 - PROGRESS: at 86.11% examples, 293407 words/s, in_qsize 16, out_qsize 0\n",
      "2020-09-20 09:18:16,880 : INFO : EPOCH 1 - PROGRESS: at 88.32% examples, 293620 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:17,917 : INFO : EPOCH 1 - PROGRESS: at 90.25% examples, 293104 words/s, in_qsize 15, out_qsize 1\n",
      "2020-09-20 09:18:18,931 : INFO : EPOCH 1 - PROGRESS: at 92.45% examples, 293347 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:19,983 : INFO : EPOCH 1 - PROGRESS: at 94.71% examples, 293478 words/s, in_qsize 15, out_qsize 0\n",
      "2020-09-20 09:18:21,036 : INFO : EPOCH 1 - PROGRESS: at 97.08% examples, 293997 words/s, in_qsize 16, out_qsize 0\n",
      "2020-09-20 09:18:22,054 : INFO : EPOCH 1 - PROGRESS: at 99.32% examples, 294089 words/s, in_qsize 11, out_qsize 0\n",
      "2020-09-20 09:18:22,178 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-20 09:18:22,197 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-20 09:18:22,205 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-20 09:18:22,207 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-20 09:18:22,219 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-20 09:18:22,241 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-20 09:18:22,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-20 09:18:22,287 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-20 09:18:22,288 : INFO : EPOCH - 1 : training on 16181598 raw words (13673009 effective words) took 46.5s, 294292 effective words/s\n",
      "2020-09-20 09:18:22,288 : INFO : training on a 16181598 raw words (13673009 effective words) took 46.5s, 294271 effective words/s\n",
      "2020-09-20 09:18:22,381 : INFO : saving Word2Vec object under /Users/sara/Documents/算法学习/NLP/nlp名企/MRC/data/wv/word2vec.model, separately None\n",
      "2020-09-20 09:18:22,382 : INFO : storing np array 'vectors' to /Users/sara/Documents/算法学习/NLP/nlp名企/MRC/data/wv/word2vec.model.wv.vectors.npy\n",
      "2020-09-20 09:18:23,273 : INFO : not storing attribute vectors_norm\n",
      "2020-09-20 09:18:23,274 : INFO : storing np array 'syn1neg' to /Users/sara/Documents/算法学习/NLP/nlp名企/MRC/data/wv/word2vec.model.trainables.syn1neg.npy\n",
      "2020-09-20 09:18:24,344 : INFO : not storing attribute cum_table\n",
      "2020-09-20 09:18:24,565 : INFO : saved /Users/sara/Documents/算法学习/NLP/nlp名企/MRC/data/wv/word2vec.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start build w2v model finished\n"
     ]
    }
   ],
   "source": [
    "# 1.加载数据\n",
    "search_dev_df = pd.read_json(search_dev_data_path, lines=True)\n",
    "zhidao_dev_df = pd.read_json(zhidao_dev_data_path,encoding='utf-8', lines=True)\n",
    "print('search dev data size {},zhidao dev data size {}'.format(len(search_dev_df), len(zhidao_dev_df)))\n",
    "\n",
    "# 2、数据处理\n",
    "search_dev_df['answers'] = search_dev_df[['answers']].apply(sentence_proc, axis=1)\n",
    "search_dev_df['entity_answers'] = search_dev_df[['entity_answers']].apply(sentences_proc, axis=1)\n",
    "search_dev_df['documents'] = search_dev_df[['documents']].apply(documents_proc, axis=1)\n",
    "zhidao_dev_df['answers'] = zhidao_dev_df[['answers']].apply(sentence_proc, axis=1)\n",
    "zhidao_dev_df['entity_answers'] = zhidao_dev_df[['entity_answers']].apply(sentences_proc, axis=1)\n",
    "zhidao_dev_df['documents'] = zhidao_dev_df[['documents']].apply(documents_proc, axis=1)\n",
    "\n",
    "# 3.多线程, 批量数据处理\n",
    "search_dev_df = parallelize(search_dev_df, split_sentences_proc)\n",
    "zhidao_dev_df = parallelize(zhidao_dev_df, split_sentences_proc)\n",
    "\n",
    "# 4. 合并训练测试集合\n",
    "search_dev_df['merged'] = search_dev_df[['documents', 'entity_answers', 'question', 'answers']].apply(lambda x: ' '.join(x), axis=1)\n",
    "zhidao_dev_df['merged'] = search_dev_df[['documents', 'entity_answers', 'question', 'answers']].apply(lambda x: ' '.join(x), axis=1)\n",
    "merged_df = pd.concat([search_dev_df[['merged']], zhidao_dev_df[['merged']]], axis=0)\n",
    "print('train data size {},test data size {},merged_df data size {}'.format(len(search_dev_df),\n",
    "                                                                           len(zhidao_dev_df),\n",
    "                                                                           len(merged_df)))\n",
    "# 6. 保存合并数据\n",
    "merged_df.to_csv(merger_dev_seg_path, index=None, header=False)\n",
    "\n",
    "# 7. 训练词向量\n",
    "print('start build w2v model')\n",
    "wv_model = Word2Vec(LineSentence(merger_dev_seg_path),\n",
    "                    size=embedding_dim,\n",
    "                    sg=1,\n",
    "                    workers=cores,\n",
    "                    iter=wv_train_epochs,\n",
    "                    window=5,\n",
    "                    min_count=5)\n",
    "# 8. 填充开始结束符号,未知词填充 oov, 长度填充\n",
    "# 使用GenSim训练得出的vocab\n",
    "vocab = wv_model.wv.vocab\n",
    "\n",
    "# 9、保存字典\n",
    "save_dict(vocab_path, vocab)\n",
    "\n",
    "# 10、保存词向量模型\n",
    "wv_model.save(save_wv_model_path)\n",
    "print('start build w2v model finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 SQuAD2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 merge\n",
      "0                              Normans\n",
      "1      Computational_complexity_theory\n",
      "2                  Southern_California\n",
      "3                 Sky_(United_Kingdom)\n",
      "4                 Victoria_(Australia)\n",
      "...                                ...\n",
      "33409                           sthène\n",
      "33410                           sthène\n",
      "33411                           sthène\n",
      "33412                           sthène\n",
      "33413                           sthène\n",
      "\n",
      "[33414 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:28:11,075 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:28:11,078 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,086 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,083 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,090 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:28:11,089 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:28:11,095 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:28:11,097 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "2020-09-20 09:28:11,101 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,108 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,115 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,122 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,135 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,133 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,143 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "2020-09-20 09:28:11,143 : DEBUG : Loading model from cache /var/folders/49/6s8tc3jx6r94lyzs0dz49dr00000gn/T/jieba.cache\n",
      "Loading model cost 1.581 seconds.\n",
      "2020-09-20 09:28:12,664 : DEBUG : Loading model cost 1.581 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:28:12,670 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.594 seconds.\n",
      "2020-09-20 09:28:12,688 : DEBUG : Loading model cost 1.594 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.570 seconds.\n",
      "2020-09-20 09:28:12,695 : DEBUG : Prefix dict has been built succesfully.\n",
      "2020-09-20 09:28:12,703 : DEBUG : Loading model cost 1.570 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:28:12,710 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.575 seconds.\n",
      "Loading model cost 1.621 seconds.\n",
      "2020-09-20 09:28:12,663 : DEBUG : Loading model cost 1.575 seconds.\n",
      "Loading model cost 1.655 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:28:12,762 : DEBUG : Loading model cost 1.621 seconds.\n",
      "2020-09-20 09:28:12,776 : DEBUG : Prefix dict has been built succesfully.\n",
      "2020-09-20 09:28:12,776 : DEBUG : Loading model cost 1.655 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:28:12,782 : DEBUG : Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:28:12,790 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.662 seconds.\n",
      "Loading model cost 1.676 seconds.\n",
      "2020-09-20 09:28:12,803 : DEBUG : Loading model cost 1.662 seconds.\n",
      "2020-09-20 09:28:12,807 : DEBUG : Loading model cost 1.676 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-09-20 09:28:12,812 : DEBUG : Prefix dict has been built succesfully.\n",
      "2020-09-20 09:28:12,816 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_df data size 33414\n",
      "                                                   merge\n",
      "0                                          N o r m a n s\n",
      "1      C o m p u t a t i o n a l   _   c o m p l e x ...\n",
      "2              S o u t h e r n   _   C a l i f o r n i a\n",
      "3      S k y   _   (   U n i t e d   _   K i n g d o ...\n",
      "4        V i c t o r i a   _   (   A u s t r a l i a   )\n",
      "...                                                  ...\n",
      "33409                                    s t h   è   n e\n",
      "33410                                    s t h   è   n e\n",
      "33411                                    s t h   è   n e\n",
      "33412                                    s t h   è   n e\n",
      "33413                                    s t h   è   n e\n",
      "\n",
      "[33414 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-20 09:28:16,539 : INFO : collecting all words and their counts\n",
      "2020-09-20 09:28:16,540 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-09-20 09:28:16,724 : INFO : PROGRESS: at sentence #10000, processed 1256206 words, keeping 325 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start build w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-20 09:28:16,778 : INFO : PROGRESS: at sentence #20000, processed 1521826 words, keeping 367 word types\n",
      "2020-09-20 09:28:16,822 : INFO : PROGRESS: at sentence #30000, processed 1714101 words, keeping 387 word types\n",
      "2020-09-20 09:28:16,837 : INFO : collected 388 word types from a corpus of 1778112 raw words and 33483 sentences\n",
      "2020-09-20 09:28:16,838 : INFO : Loading a fresh vocabulary\n",
      "2020-09-20 09:28:16,839 : INFO : effective_min_count=5 retains 243 unique words (62% of original 388, drops 145)\n",
      "2020-09-20 09:28:16,840 : INFO : effective_min_count=5 leaves 1777878 word corpus (99% of original 1778112, drops 234)\n",
      "2020-09-20 09:28:16,846 : INFO : deleting the raw counts dictionary of 388 items\n",
      "2020-09-20 09:28:16,847 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2020-09-20 09:28:16,848 : INFO : downsampling leaves estimated 415782 word corpus (23.4% of prior 1777878)\n",
      "2020-09-20 09:28:16,849 : INFO : estimated required memory for 243 words and 300 dimensions: 704700 bytes\n",
      "2020-09-20 09:28:16,850 : INFO : resetting layer weights\n",
      "2020-09-20 09:28:16,922 : INFO : training model with 8 workers on 243 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-09-20 09:28:17,926 : INFO : EPOCH 1 - PROGRESS: at 75.19% examples, 378346 words/s, in_qsize 14, out_qsize 1\n",
      "2020-09-20 09:28:17,955 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-20 09:28:17,957 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-20 09:28:17,960 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-20 09:28:17,961 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-20 09:28:17,971 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-20 09:28:17,972 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-20 09:28:17,975 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-20 09:28:17,976 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-20 09:28:17,977 : INFO : EPOCH - 1 : training on 1778112 raw words (415792 effective words) took 1.1s, 395481 effective words/s\n",
      "2020-09-20 09:28:17,978 : INFO : training on a 1778112 raw words (415792 effective words) took 1.1s, 394101 effective words/s\n",
      "2020-09-20 09:28:17,985 : INFO : saving Word2Vec object under /Users/sara/Documents/算法学习/NLP/nlp名企/MRC/data/wv/squad2_word2vec.model, separately None\n",
      "2020-09-20 09:28:17,986 : INFO : not storing attribute vectors_norm\n",
      "2020-09-20 09:28:17,987 : INFO : not storing attribute cum_table\n",
      "2020-09-20 09:28:17,996 : INFO : saved /Users/sara/Documents/算法学习/NLP/nlp名企/MRC/data/wv/squad2_word2vec.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start build w2v model finished\n"
     ]
    }
   ],
   "source": [
    "# 1、读取数据\n",
    "squad_dev_df = pd.read_json(squad_dev_data_path, encoding='utf-8', lines=True)\n",
    "squad_dev_df = squad_dev_df['data']\n",
    "\n",
    "# 2、数据处理\n",
    "title = []\n",
    "context = []\n",
    "question = []\n",
    "answer = []\n",
    "merge = []\n",
    "for datas in squad_dev_df:\n",
    "    for data in datas:\n",
    "        title.append(data['title'])\n",
    "        for paragraph in data['paragraphs']:\n",
    "            context.append(paragraph['context'])\n",
    "            for qus in paragraph['qas']:\n",
    "                question.append(qus['question'])\n",
    "                for answers in  qus['answers']:\n",
    "                    answer.append(answers['text'])\n",
    "\n",
    "# 3、合并数据\n",
    "merge = title + context + question + answer\n",
    "squad_merge_df = pd.DataFrame(merge,columns=['merge']) \n",
    "print(squad_merge_df)\n",
    "# 4.多线程, 批量数据处理\n",
    "squad_merge_df = parallelize(squad_merge_df, en_split_sentences_proc)\n",
    "print('merge_df data size {}'.format(len(squad_merge_df)))\n",
    "print(squad_merge_df)\n",
    "# 5. 保存合并数据\n",
    "squad_merge_df.to_csv(squad2_merger_dev_seg_path, index=None, header=False)\n",
    "\n",
    "# 6. 训练词向量\n",
    "print('start build w2v model')\n",
    "wv_model = Word2Vec(LineSentence(squad2_merger_dev_seg_path),\n",
    "                    size=embedding_dim,\n",
    "                    sg=1,\n",
    "                    workers=cores,\n",
    "                    iter=wv_train_epochs,\n",
    "                    window=5,\n",
    "                    min_count=5)\n",
    "\n",
    "# 7、使用GenSim训练得出的vocab\n",
    "vocab = wv_model.wv.vocab\n",
    "\n",
    "# 8、保存字典\n",
    "save_dict(squad2_vocab_path, vocab)\n",
    "\n",
    "# 9、保存词向量模型\n",
    "wv_model.save(save_squad2_wv_model_path)\n",
    "print('start build w2v model finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
